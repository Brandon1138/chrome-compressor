{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Entropy Estimation Demo\n",
        "\n",
        "This notebook demonstrates training unigram and n-gram models to estimate entropy (bits per character) for English and Romanian text.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %load_ext autoreload\n",
        "# %autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "\n",
        "from reducelang.models import UnigramModel, NGramModel\n",
        "from reducelang.alphabet import ENGLISH_ALPHABET, ROMANIAN_ALPHABET\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Shannon's entropy rate H measures the average information per symbol. For a unigram model, H_1 = -Σ p(c) log₂ p(c). For n-gram models, H_n captures dependencies between characters.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "corpus_file = Path(\"data/corpora/en/2025-10-01/processed/text8.txt\")\n",
        "text = corpus_file.read_text(encoding=\"utf-8\")\n",
        "print(f\"Corpus size: {len(text)} chars\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "split_idx = int(len(text) * 0.8)\n",
        "train_text = text[:split_idx]\n",
        "test_text = text[split_idx:]\n",
        "print(f\"Train: {len(train_text)}, Test: {len(test_text)}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Unigram model demonstration.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unigram = UnigramModel(ENGLISH_ALPHABET)\n",
        "unigram.fit(train_text)\n",
        "H1 = unigram.evaluate(test_text)\n",
        "print(f\"Unigram H_1: {H1:.4f} bits/char\")\n",
        "print(f\"Max entropy (uniform): {ENGLISH_ALPHABET.log2_size:.4f} bits/char\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "N-gram models with different orders.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "orders = [2, 3, 5, 8]\n",
        "results = {}\n",
        "for order in orders:\n",
        "    model = NGramModel(ENGLISH_ALPHABET, order=order)\n",
        "    model.fit(train_text)\n",
        "    H = model.evaluate(test_text)\n",
        "    results[order] = H\n",
        "    print(f\"N-gram (order={order}): {H:.4f} bits/char\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 5))\n",
        "plt.plot(orders, [results[o] for o in orders], marker='o')\n",
        "plt.axhline(H1, color='red', linestyle='--', label='Unigram')\n",
        "plt.xlabel('N-gram order')\n",
        "plt.ylabel('Cross-entropy (bits/char)')\n",
        "plt.title('Entropy vs. N-gram Order')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redundancy R = 1 - H / log₂M. For English with M=27, if H≈1.5 bits/char, then R ≈ 1 - 1.5/4.755 ≈ 68%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "M = ENGLISH_ALPHABET.size\n",
        "log2M = ENGLISH_ALPHABET.log2_size\n",
        "for order, H in results.items():\n",
        "    R = 1 - H / log2M\n",
        "    print(f\"Order {order}: H={H:.4f}, R={R:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Higher-order n-grams capture more dependencies, reducing entropy and increasing measured redundancy. This demonstrates Shannon's insight that natural language has significant redundancy due to predictable patterns.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = {\"model\": \"ngram\", \"orders\": orders, \"entropy\": results}\n",
        "with open(\"entropy_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "print(\"Results saved to entropy_results.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
