{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# PPM Entropy Estimation Demo\n",
        "\n",
        "This notebook demonstrates Prediction by Partial Matching (PPM) for entropy estimation, including escape methods, context blending, and codelength verification.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "from pathlib import Path\n",
        "import json\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from reducelang.models import PPMModel, UnigramModel, NGramModel\n",
        "from reducelang.alphabet import ENGLISH_ALPHABET\n",
        "from reducelang.coding import verify_codelength\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPM uses variable-length contexts to predict the next character. It blends probabilities from multiple context lengths using an escape mechanism. Longer contexts provide better predictions when available; shorter contexts provide fallback estimates.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load a sample corpus (adjust path as needed)\n",
        "corpus_file = Path(\"data/corpora/en/2025-10-01/processed/text8.txt\")\n",
        "text = corpus_file.read_text(encoding=\"utf-8\")[:100_000]\n",
        "split_idx = int(len(text) * 0.8)\n",
        "train_text = text[:split_idx]\n",
        "test_text = text[split_idx:]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Train PPM models with varying depths to see how entropy changes.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "depths = [1, 2, 3, 5, 8]\n",
        "ppm_results = {}\n",
        "for depth in depths:\n",
        "    model = PPMModel(ENGLISH_ALPHABET, depth=depth, escape_method=\"A\")\n",
        "    model.fit(train_text)\n",
        "    H = model.evaluate(test_text)\n",
        "    ppm_results[depth] = H\n",
        "    print(f\"PPM (depth={depth}): {H:.4f} bits/char\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "unigram = UnigramModel(ENGLISH_ALPHABET)\n",
        "unigram.fit(train_text)\n",
        "H_unigram = unigram.evaluate(test_text)\n",
        "print(f\"Unigram: {H_unigram:.4f} bits/char\")\n",
        "\n",
        "ngram5 = NGramModel(ENGLISH_ALPHABET, order=5)\n",
        "ngram5.fit(train_text)\n",
        "H_ngram5 = ngram5.evaluate(test_text)\n",
        "print(f\"N-gram (order=5): {H_ngram5:.4f} bits/char\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(depths, [ppm_results[d] for d in depths], marker='o', label='PPM')\n",
        "plt.axhline(H_unigram, color='red', linestyle='--', label='Unigram')\n",
        "plt.axhline(H_ngram5, color='green', linestyle='--', label='N-gram (order=5)')\n",
        "plt.xlabel('PPM Depth')\n",
        "plt.ylabel('Cross-entropy (bits/char)')\n",
        "plt.title('Entropy vs. PPM Depth')\n",
        "plt.legend()\n",
        "plt.grid()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "To verify that our entropy estimates are correct, we use an arithmetic coder to compute the actual codelength. The codelength should match the cross-entropy within a small tolerance (< 0.001 bpc).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "ppm8 = PPMModel(ENGLISH_ALPHABET, depth=8, escape_method=\"A\")\n",
        "ppm8.fit(train_text)\n",
        "verification = verify_codelength(test_text, ppm8, tolerance=1e-3)\n",
        "print(f\"Cross-entropy: {verification['cross_entropy_bpc']:.6f} bpc\")\n",
        "print(f\"Codelength:    {verification['codelength_bpc']:.6f} bpc\")\n",
        "print(f\"Delta:         {verification['delta_bpc']:.6f} bpc\")\n",
        "print(f\"Matches:       {verification['matches']}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPM supports different escape methods (A, B, C, D) that allocate escape probability differently. Method A (default) is simplest: escape probability = c / (n + c), where c = unique symbols seen, n = total count.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "escape_methods = [\"A\", \"B\", \"C\", \"D\"]\n",
        "escape_results = {}\n",
        "for method in escape_methods:\n",
        "    try:\n",
        "        model = PPMModel(ENGLISH_ALPHABET, depth=5, escape_method=method)\n",
        "        model.fit(train_text)\n",
        "        H = model.evaluate(test_text)\n",
        "        escape_results[method] = H\n",
        "        print(f\"Escape method {method}: {H:.4f} bits/char\")\n",
        "    except NotImplementedError:\n",
        "        print(f\"Escape method {method}: Not implemented\")\n",
        "        escape_results[method] = None\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Redundancy R = 1 - H / log₂M. For English with M=27, if PPM gives H≈1.3 bits/char, then R ≈ 1 - 1.3/4.755 ≈ 73%.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "M = ENGLISH_ALPHABET.size\n",
        "log2M = ENGLISH_ALPHABET.log2_size\n",
        "for depth, H in ppm_results.items():\n",
        "    R = 1.0 - (H / log2M)\n",
        "    print(f\"PPM (depth={depth}): H={H:.4f}, R={R:.2%}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "PPM achieves lower entropy than n-grams by using adaptive context selection. Deeper contexts capture more dependencies, approaching Shannon's theoretical limit. Codelength verification confirms our entropy estimates are accurate.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "output = {\"model\": \"ppm\", \"depths\": depths, \"entropy\": ppm_results, \"verification\": verification}\n",
        "with open(\"ppm_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
        "    json.dump(output, f, indent=2)\n",
        "print(\"Results saved to ppm_results.json\")\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
