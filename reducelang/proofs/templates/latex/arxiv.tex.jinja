% Auto-generated LaTeX proof document (arXiv-friendly article)
\documentclass[11pt]{article}

% Packages and layout
\usepackage[margin=1in]{geometry}
\usepackage{amsmath, amsthm, amssymb}
\usepackage{graphicx}
\usepackage{booktabs}
\usepackage[colorlinks=true, linkcolor=blue, citecolor=blue]{hyperref}
\usepackage[numbers]{natbib}

% Theorem environments
\newtheorem{theorem}{Theorem}
\newtheorem{proposition}{Proposition}
\newtheorem{definition}{Definition}

% Metadata
\title{Shannon Redundancy Estimation for {{ language_name | escape_latex }}}
\author{Generated by reducelang}
\date{\today}

\begin{document}
\maketitle

\begin{abstract}
We estimate the entropy rate of {{ language_name | escape_latex }} text and the implied Shannon redundancy using statistical models (unigram, n-gram, PPM) and compression baselines (Huffman). Our best model achieves $H \approx {{ '%.4f'|format(ppm_bpc) }}$ bpc, implying redundancy $R \approx {{ '%.2f'|format(ppm_redundancy * 100) }}\%$.
\end{abstract}

\section{Introduction}
Shannon's framework quantifies the information content of language and defines redundancy as the fraction of predictable information that can be compressed away. We target the classic $\sim 68\%$ redundancy estimate for English and extend the analysis to other languages.

\section{Theoretical Framework}
\begin{definition}[Entropy Rate]
The entropy rate $H$ of a stochastic process is
\[ H = \lim_{n\to\infty} \frac{1}{n} H(X_1, \ldots, X_n). \]
\end{definition}

\begin{definition}[Redundancy]
Given alphabet size $M$, $\log_2 M = {{ '%.3f'|format(log2_alphabet_size) }}$ bpc, redundancy is $R = 1 - H/\log_2 M$.
\end{definition}

\begin{theorem}[Source Coding Theorem]
For a stationary ergodic source, optimal compression achieves rate arbitrarily close to $H$ bits per symbol.
\end{theorem}

\begin{proposition}[Finite-Order Bounds]
The $n$-gram entropies $H_n$ monotonically decrease to $H$ as $n\to\infty$.
\end{proposition}

\section{Methodology}
Alphabet $M={{ alphabet_size }}$, $\log_2 M={{ '%.3f'|format(log2_alphabet_size) }}$ bpc. Models: unigram, n-gram (2--8), PPM (3--8), Huffman. Corpus: {{ corpus_name | escape_latex }} ({{ corpus_size }} chars). Train/test split 80/20. Bootstrap: 1000 resamples (95\% CI). Sensitivity: {{ ablations_list | escape_latex }}.

\section{Results}
\subsection*{Comparison Table}
\begin{table}[h]
\centering
\begin{tabular}{lrrrrr}
\toprule
Model & Order & H (bpc) & $\log_2 M$ & Redundancy (\%) & Comp. Ratio \\
\midrule
{% for row in comparison_table_data %}
{{ row.model | escape_latex }} & {{ row.order }} & {{ '%.4f'|format(row.bits_per_char) }} & {{ '%.4f'|format(row.log2_alphabet_size) }} & {{ '%.2f'|format(row.redundancy * 100) }} & {{ '%.3f'|format(row.compression_ratio) }} \\
{% endfor %}
\bottomrule
\end{tabular}
\caption{Model comparison on {{ language_name | escape_latex }} / {{ corpus_name | escape_latex }}.}
\label{tab:comparison}
\end{table}

\subsection*{Figures}
\begin{figure}[h]
\centering
\includegraphics[width=0.8\textwidth]{ {{ figs_rel_path_tex }}/entropy_vs_order_{{ lang }}.pdf }
\caption{Entropy vs. model order/depth. Horizontal dashed line shows $\log_2 M$.}
\end{figure}

\subsection*{Key Findings}
PPM (depth={{ ppm_order }}) achieves $H = {{ '%.4f'|format(ppm_bpc) }} \pm {{ '%.4f'|format(ppm_ci_width) }}$ bpc (95\% CI), $R = {{ '%.2f'|format(ppm_redundancy * 100) }}\% \pm {{ '%.2f'|format(redundancy_ci_width * 100) }}\%$. Huffman baseline: $H = {{ '%.4f'|format(huffman_bpc) }}$ bpc, $R = {{ '%.2f'|format(huffman_redundancy * 100) }}\%$. PPM captures {{ '%.2f'|format(ppm_to_huffman_gain) }} percentage points more redundancy than Huffman.

\section{Sensitivity Analysis}
{% if has_sensitivity %}
\begin{table}[h]
\centering
\begin{tabular}{lrrrrrr}
\toprule
Variant & $M$ & $\log_2 M$ & H (bpc) & Redundancy (\%) & $\Delta H$ & $\Delta R$ \\
\midrule
{% for row in sensitivity_table_data %}
{{ row.variant | escape_latex }} & {{ row.alphabet_size }} & {{ '%.3f'|format(row.log2_alphabet_size) }} & {{ '%.4f'|format(row.bits_per_char) }} & {{ '%.2f'|format(row.redundancy * 100) }} & {{ '%.4f'|format(row.delta_h) }} & {{ '%.2f'|format(row.delta_r * 100) }} \\
{% endfor %}
\bottomrule
\end{tabular}
\caption{Sensitivity to alphabet and preprocessing ablations.}
\end{table}
{% else %}
No sensitivity results available.
{% endif %}

\section{Discussion}
We compare our estimates with Shannon's original results (\cite{shannon1951}) and discuss implications for compression and NLP. Limitations include finite corpora and model assumptions.

\section{Conclusion}
We present a template-driven approach to generating publication-ready proofs for redundancy estimation.

\bibliographystyle{plainnat}
\bibliography{references}

\end{document}


