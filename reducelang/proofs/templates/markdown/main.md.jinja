---
title: "Shannon Redundancy Estimation for {{ language_name }}"
author: "Generated by reducelang"
date: "{{ generation_date }}"
format:
  html:
    toc: true
    toc-depth: 3
    code-fold: true
    theme: cosmo
    mathjax: true
---

## Abstract

We estimate the entropy rate of {{ language_name }} text and the implied Shannon redundancy using statistical models (unigram, n-gram, PPM) and compression baselines (Huffman). Our best model achieves $H \approx {{ '%.4f'|format(ppm_bpc) }}$ bpc, implying redundancy $R \approx {{ '%.2f'|format(ppm_redundancy * 100) }}\%$.

## Introduction

Shannon's framework quantifies the information content of language and defines redundancy as the fraction of predictable information that can be compressed away.

## Theoretical Framework

::: {.callout-note icon=false}
### Definition 1 (Entropy Rate)
The entropy rate $H$ of a stochastic process is defined as:
$$H = \lim_{n \to \infty} \frac{1}{n} H(X_1, \ldots, X_n).$$
:::

::: {.callout-note icon=false}
### Definition 2 (Redundancy)
For alphabet size $M$ with $\log_2 M = {{ '%.3f'|format(log2_alphabet_size) }}$ bpc, redundancy is $R = 1 - H/\log_2 M$.
:::

::: {.callout-note icon=false}
### Theorem 1 (Source Coding Theorem)
Optimal compression achieves $H$ bits per symbol for stationary ergodic sources.
:::

::: {.callout-note icon=false}
### Proposition 1 (Finite-Order Bounds)
The $n$-gram entropies $H_n$ decrease to $H$ as $n\to\infty$.
:::

::: {.callout-note icon=false}
### Proposition 2 (Shannon's Guessing Game)
Human prediction experiments bound $H$ via average number of guesses per symbol.
:::

## Methodology

- **Alphabet**: $M={{ alphabet_size }}$, $\log_2 M={{ '%.3f'|format(log2_alphabet_size) }}$ bpc
- **Models**: unigram, n-gram (2–8), PPM (3–8), Huffman
- **Corpus**: {{ corpus_name }} ({{ corpus_size }})
- **Split**: 80/20 train/test
- **Bootstrap**: 1000 resamples (95% CI)
- **Sensitivity**: {{ ablations_list }}

## Results

### Table 1: Model Comparison

| Model | Order | H (bpc) | log₂M | Redundancy | Comp. Ratio |
|-------|-------|---------|-------|------------|-------------|
{% for row in comparison_table_data %}
| {{ row.model }} | {{ row.order }} | {{ "%.4f"|format(row.bits_per_char) }} | {{ "%.4f"|format(row.log2_alphabet_size) }} | {{ "%.2f%%"|format(row.redundancy * 100) }} | {{ "%.3f"|format(row.compression_ratio) }} |
{% endfor %}

### Figure 1: Entropy vs. order

![Entropy vs. Order]({{ figs_rel_path_md }}/entropy_vs_order_{{ lang }}.png)

### Key findings

PPM (depth={{ ppm_order }}) achieves $H = {{ '%.4f'|format(ppm_bpc) }} \pm {{ '%.4f'|format(ppm_ci_width) }}$ bpc (95% CI), $R = {{ '%.2f'|format(ppm_redundancy * 100) }}\% \pm {{ '%.2f'|format(redundancy_ci_width * 100) }}\%$.

### Huffman baseline

$H = {{ '%.4f'|format(huffman_bpc) }}$ bpc, $R = {{ '%.2f'|format(huffman_redundancy * 100) }}\%$.

### Redundancy gain

PPM captures {{ '%.2f'|format(ppm_to_huffman_gain) }} percentage points more redundancy than Huffman.

## Sensitivity Analysis

{% if has_sensitivity %}
| Variant | M | log₂M | H (bpc) | Redundancy | ΔH | ΔR |
|---------|---|-------|---------|------------|----|----|
{% for row in sensitivity_table_data %}
| {{ row.variant }} | {{ row.alphabet_size }} | {{ '%.3f'|format(row.log2_alphabet_size) }} | {{ '%.4f'|format(row.bits_per_char) }} | {{ '%.2f%%'|format(row.redundancy * 100) }} | {{ '%.4f'|format(row.delta_h) }} | {{ '%.2f%%'|format(row.delta_r * 100) }} |
{% endfor %}
{% else %}
No sensitivity results available.
{% endif %}

## Discussion

We compare with Shannon's original estimate (~68% for English) and discuss implications for compression and NLP.

## Conclusion

Template-driven generation of publication-ready reports from results.

## References

1. Shannon, C. E. (1951). [Prediction and Entropy of Printed English](https://www.princeton.edu/~wbialek/rome/refs/shannon_51.pdf).
2. Huffman, D. A. (1952). [A Method for the Construction of Minimum-Redundancy Codes](https://compression.ru/download/articles/huff/huffman_1952_minimum-redundancy-codes.pdf).
3. Cover, T. M., & Thomas, J. A. [Elements of Information Theory](https://staff.ustc.edu.cn/~cgong821/Wiley.Interscience.Elements.of.Information.Theory.Jul.2006.eBook-DDU.pdf).
4. Cleary, J. G., & Witten, I. H. (1984). Data compression using adaptive coding and partial string matching.
5. Moffat, A. (1990). Implementing the PPM data compression scheme.
6. Teahan, W. J., & Cleary, J. G. (1997). The entropy of English using PPM-based models.
7. Efron, B., & Tibshirani, R. J. (1993). An Introduction to the Bootstrap.
8. Berg-Kirkpatrick, T., Burkett, D., & Klein, D. (2012). An Empirical Investigation of Statistical Significance in NLP.


